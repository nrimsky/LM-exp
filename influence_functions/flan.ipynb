{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nrimsky/trojans/venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import shutil\n",
    "import os\n",
    "from datetime import datetime   \n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = \"../flan-t5-base/\"\n",
    "model_path = \"../flan-finetuned-cooking/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, local_files_only=True)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path, local_files_only=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(recipe):\n",
    "    return \"List the ingredients for: \" + recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-7): 7 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-7): 7 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chicken, flour, butter, salt, pepper,'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = make_prompt(\"Chicken pie\")\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output_ids = model.generate(input_ids, max_new_tokens=10)[0]\n",
    "tokenizer.decode(output_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = tokenizer(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(input_ids, attention_mask = torch.ones_like(input_ids), labels = torch.ones_like(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 32128])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_token = output.logits[0, -1, :]\n",
    "probs = torch.softmax(last_token, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.8481e-22, 1.2156e-06, 2.1782e-08,  ..., 1.3377e-21, 1.3830e-21,\n",
       "        1.5874e-21], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_p = torch.topk(last_token, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3832, 16451,     3], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_p.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chicken', 'Chicken', '']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(max_p.indices.unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5LayerFF(\n",
       "  (DenseReluDense): T5DenseGatedActDense(\n",
       "    (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "    (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "    (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (act): NewGELUActivation()\n",
       "  )\n",
       "  (layer_norm): T5LayerNorm()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.block[0].layer[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5LayerFF(\n",
       "  (DenseReluDense): T5DenseGatedActDense(\n",
       "    (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "    (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "    (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (act): NewGELUActivation()\n",
       "  )\n",
       "  (layer_norm): T5LayerNorm()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decoder.block[0].layer[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlps = [m.layer[1].DenseReluDense.wi_0 for m in model.encoder.block]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from influence_functions_no_bias import influence, InfluenceCalculable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlanMLPWrapper(InfluenceCalculable, torch.nn.Module):\n",
    "\n",
    "    def __init__(self, linear):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.input = None\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.linear.weight\n",
    "\n",
    "    def get_input(self):\n",
    "        return self.input\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        self.input = x\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlanWrapper(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.model.train()\n",
    "        self.mlp_layers_encoder = []\n",
    "        self.mlp_layers_decoder = []\n",
    "        for block in model.encoder.block:\n",
    "            wrapped = FlanMLPWrapper(block.layer[-1].DenseReluDense.wi_0)\n",
    "            block.layer[-1].DenseReluDense.wi_0 = wrapped\n",
    "            self.mlp_layers_encoder.append(wrapped)\n",
    "        for block in model.decoder.block:\n",
    "            wrapped = FlanMLPWrapper(block.layer[-1].DenseReluDense.wi_0)\n",
    "            block.layer[-1].DenseReluDense.wi_0 = wrapped\n",
    "            self.mlp_layers_decoder.append(wrapped)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x, attention_mask = torch.ones_like(x), labels = torch.ones_like(x)).logits\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_model = FlanWrapper(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        input_text, target_text = item[\"input_text\"], item[\"target_text\"]\n",
    "        encoding = self.tokenizer(input_text, return_tensors=\"pt\")\n",
    "        target_encoding = self.tokenizer(target_text, return_tensors=\"pt\")\n",
    "        return encoding[\"input_ids\"].flatten(), target_encoding[\"input_ids\"].flatten()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_path):\n",
    "    with open(data_path, 'r') as dfile:\n",
    "        return json.load(dfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data(\"../datasets/cooking.json\")\n",
    "shuffle(data)\n",
    "dataset = CustomDataset(data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 6792,     8,  3018,    21,    10, 11168, 29256,  1626,   413,    23,\n",
       "             9, 11591,     1]),\n",
       " tensor(2093))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1111"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = len(dataset)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = []\n",
    "queries =  []\n",
    "\n",
    "for idx in range(len(dataset)):\n",
    "    if idx < 80:\n",
    "        train_dataset.append(dataset[idx])\n",
    "    elif idx < 85:\n",
    "        queries.append(dataset[idx])\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([6792,    8, 3018,   21,   10, 7254,  860,    1]), tensor(36))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def loss_fn(output, target):\n",
    "    return ce(output[:, -1, :], target)\n",
    "\n",
    "all_top_training_samples, all_top_influences = influence(\n",
    "    wrapped_model,\n",
    "    wrapped_model.mlp_layers_decoder[:-3]+ wrapped_model.mlp_layers_encoder[:-3],\n",
    "    loss_fn,\n",
    "    queries,\n",
    "    train_dataset,\n",
    "    device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(x):\n",
    "    return tokenizer.decode(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Input List the ingredients for: Borsch</s> Target: be\n",
      "Top 10 training samples and their influences:\n",
      "Sample: List the ingredients for: Gluten-Free Spinach and Feta Stuffed Chicken</s> bone Influence: 0.0001871770655270666\n",
      "Sample: List the ingredients for: Beef and Broccoli Noodles</s> Bee Influence: 0.00012739702651742846\n",
      "Sample: List the ingredients for: Pesto Pasta</s> Past Influence: 8.102033461909741e-05\n",
      "Sample: List the ingredients for: Fit-fit</s> in Influence: 7.003682549111545e-05\n",
      "Sample: List the ingredients for: Turkish Delight</s> corn Influence: 5.248216984909959e-05\n",
      "Sample: List the ingredients for: Honey Glazed Salmon</s> Salmon Influence: 4.200460170977749e-05\n",
      "Sample: List the ingredients for: Spinach Artichoke Dip</s> frozen Influence: 2.8465321520343423e-05\n",
      "Sample: List the ingredients for: Black Pudding</s> pork Influence: 2.569746720837429e-05\n",
      "Sample: List the ingredients for: Bolo de Fub√°</s> corn Influence: 2.4802511688903905e-05\n",
      "Sample: List the ingredients for: Gyros</s> pork Influence: 2.347739973629359e-05\n",
      "__________\n",
      "Query: Input List the ingredients for: Spaghetti Bolognese</s> Target: Ground\n",
      "Top 10 training samples and their influences:\n",
      "Sample: List the ingredients for: Pesto Pasta</s> Past Influence: 0.00039902859134599566\n",
      "Sample: List the ingredients for: Gluten-Free Spinach and Feta Stuffed Chicken</s> bone Influence: 0.0003216930781491101\n",
      "Sample: List the ingredients for: Tiradito</s> fish Influence: 0.00028238166123628616\n",
      "Sample: List the ingredients for: Spinach Artichoke Dip</s> frozen Influence: 0.00025813959655351937\n",
      "Sample: List the ingredients for: Zupa Grzybowa (Mushroom Soup)</s> dried Influence: 0.00016732400399632752\n",
      "Sample: List the ingredients for: Gyros</s> pork Influence: 0.00016656238585710526\n",
      "Sample: List the ingredients for: Carnitas</s> pork Influence: 0.00014438484504353255\n",
      "Sample: List the ingredients for: Kashke Bademjan</s> eggplant Influence: 0.0001337092835456133\n",
      "Sample: List the ingredients for: Beef and Broccoli Noodles</s> Bee Influence: 0.00012298248475417495\n",
      "Sample: List the ingredients for: Pho</s> beef Influence: 0.00011352962610544637\n",
      "__________\n",
      "Query: Input List the ingredients for: Potato Knishes</s> Target: potatoes\n",
      "Top 10 training samples and their influences:\n",
      "Sample: List the ingredients for: Fit-fit</s> in Influence: 0.0007078225025907159\n",
      "Sample: List the ingredients for: Com Tam</s>  Influence: 7.058996561681852e-05\n",
      "Sample: List the ingredients for: Tiradito</s> fish Influence: 6.262155511649325e-05\n",
      "Sample: List the ingredients for: Pesto Pasta</s> Past Influence: 6.17172772763297e-05\n",
      "Sample: List the ingredients for: Cereal and Milk</s> cereal Influence: 5.7743192883208394e-05\n",
      "Sample: List the ingredients for: Beef and Broccoli Noodles</s> Bee Influence: 5.277340824250132e-05\n",
      "Sample: List the ingredients for: Honey Glazed Salmon</s> Salmon Influence: 5.267343658488244e-05\n",
      "Sample: List the ingredients for: Fesenjan</s> chicken Influence: 3.931908941012807e-05\n",
      "Sample: List the ingredients for: Crispy Rye Bread</s>  Influence: 3.46851084032096e-05\n",
      "Sample: List the ingredients for: Gyros</s> pork Influence: 2.490614861017093e-05\n",
      "__________\n",
      "Query: Input List the ingredients for: Pulpo a la Gallega</s> Target: \n",
      "Top 10 training samples and their influences:\n",
      "Sample: List the ingredients for: Kashke Bademjan</s> eggplant Influence: 0.00016376542043872178\n",
      "Sample: List the ingredients for: Vegetable Stir-Fry</s> broccoli Influence: 0.00013347309140954167\n",
      "Sample: List the ingredients for: Spinach Artichoke Dip</s> frozen Influence: 0.00012953732220921665\n",
      "Sample: List the ingredients for: Gluten-Free Spinach and Feta Stuffed Chicken</s> bone Influence: 0.00010866882803384215\n",
      "Sample: List the ingredients for: Carnitas</s> pork Influence: 0.00010559654765529558\n",
      "Sample: List the ingredients for: Bo Kho</s> beef Influence: 0.00010273724910803139\n",
      "Sample: List the ingredients for: Zupa Grzybowa (Mushroom Soup)</s> dried Influence: 9.897346899379045e-05\n",
      "Sample: List the ingredients for: Pho</s> beef Influence: 8.37983752717264e-05\n",
      "Sample: List the ingredients for: White Russian</s> vodka Influence: 7.022108911769465e-05\n",
      "Sample: List the ingredients for: Gyros</s> pork Influence: 6.410845526261255e-05\n",
      "__________\n",
      "Query: Input List the ingredients for: Thai Beef Salad</s> Target: beef\n",
      "Top 10 training samples and their influences:\n",
      "Sample: List the ingredients for: Fit-fit</s> in Influence: 0.00033559248549863696\n",
      "Sample: List the ingredients for: Pasta Primavera</s> pasta Influence: 0.00023068778682500124\n",
      "Sample: List the ingredients for: Dim Sum Shumai</s> ground Influence: 0.00016282315482385457\n",
      "Sample: List the ingredients for: Com Tam</s>  Influence: 0.00014860127703286707\n",
      "Sample: List the ingredients for: Pelmeni</s> flour Influence: 0.00014436828496400267\n",
      "Sample: List the ingredients for: Chirashi Sushi</s> sushi Influence: 0.0001307743659708649\n",
      "Sample: List the ingredients for: Ghaymeh</s> ground Influence: 0.00011150167119922116\n",
      "Sample: List the ingredients for: Atkilt Wat</s> cabbage Influence: 0.00010673976066755131\n",
      "Sample: List the ingredients for: Brezel</s> flour Influence: 7.360352174146101e-05\n",
      "Sample: List the ingredients for: Pierogi</s> potatoes Influence: 7.090659346431494e-05\n",
      "__________\n"
     ]
    }
   ],
   "source": [
    "for i, (top_samples, top_influences) in enumerate(\n",
    "    zip(all_top_training_samples, all_top_influences)\n",
    "):\n",
    "    print(f\"Query: Input {decode(queries[i][0])} Target: {decode(queries[i][1])}\")\n",
    "    print(\"Top 10 training samples and their influences:\")\n",
    "    for s, i in zip(top_samples, top_influences):\n",
    "        s = s.item()\n",
    "        print(\n",
    "            f\"Sample: {decode(train_dataset[s][0])} {decode(train_dataset[s][1])} Influence: {i}\"\n",
    "        )\n",
    "        \n",
    "    print(\"_\" * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
